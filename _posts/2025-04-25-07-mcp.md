---
layout: post
title:  "7. Model Context Protocol"
author: skalavala
image: assets/images/mcp.png
categories: [ RAG, AI, tools, mcp ]
tags: [ rag, llm, tools, mcp ]
featured: true
hidden: false
---

Language models are evolving fast, but there is still one major bottleneck:
context handling and tool use.

Until recently, traditional tool-calling methods, like OpenAI’s function-calling or LangChain’s tool chaining, involved hardcoding how models interact with external APIs. Every tool call needed heavy prompting, tight coupling between instructions and schemas, and lots of fragile orchestration.

Model Context Protocol (MCP) offers a cleaner way forward.

MCP makes tool use, memory access, knowledge retrieval, and action planning structured and dynamic.
Instead of cramming everything into giant prompts, models interact with external systems through lightweight, organized APIs. Before we go deeper, lets understand a bit more on the shortcomings of plain old tool calling.

In the traditional tool-calling setup:

Tools must be registered manually inside prompts. This makes the consuming application and the source system (or target system, depending on how you see this) coupled tightly with each other.

Context must be squashed into the system message or conversation history.

Models have no idea what tools exist unless you re-feed that knowledge every time.

JSON-based function calling is rigid; if a tool schema changes, the whole setup can break.

This does not scale well when you have multiple tools, dynamic APIs, agentic workflows, or real-time changes.

### How MCP is a game changer
MCP introduces a server that acts as a live context manager. The server holds the available tools, memory, and actions. The model can query the server as needed instead of memorizing everything inside the prompt.

Key differences:

Dynamic Discovery: Models ask for available tools at runtime.

Structured Context: Memory, tools, and actions are handled separately from the prompt, reducing clutter. Fetching memory and tool information externally saves large prompt overhead. 

Adaptability: Models can adjust when tools change without needing new prompts or retraining. New tools, memories, or knowledge sources can be added without touching the model instructions

Scalable Planning: Agents can fetch plans, update goals, and reason with fresh context rather than working from stale memory. This is absolutely critical when building agentic systems.  

Sample MCP Server:

```python
from fastapi import FastAPI
from typing import Dict
import uvicorn

app = FastAPI()

tools = {
    "get_weather": {
        "description": "Fetches current weather for a location",
        "input_schema": {"location": "str"}
    },
    "search_wikipedia": {
        "description": "Searches Wikipedia for a given query",
        "input_schema": {"query": "str"}
    }
}

@app.get("/mcp/tools")
async def list_tools() -> Dict:
    return tools

@app.post("/mcp/invoke")
async def invoke_tool(tool_name: str, params: dict):
    if tool_name == "get_weather":
        return {"result": f"Weather in {params['location']} is sunny."}
    if tool_name == "search_wikipedia":
        return {"result": f"Wikipedia search result for {params['query']}."}
    return {"error": "Unknown tool"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

```

To consume the MCP Server above, the client code looks like below:

```python
import requests

tools = requests.get("http://localhost:8000/mcp/tools").json()
print("Available tools:", tools.keys())

response = requests.post(
    "http://localhost:8000/mcp/invoke",
    params={"tool_name": "get_weather"},
    json={"params": {"location": "San Francisco"}}
)

print(response.json())
```
The model or agent does not need a giant static prompt anymore. It queries the server when it needs information or tools and acts accordingly.

